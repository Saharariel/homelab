apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-21T16:46:29Z"
    generateName: coredns-ccb96694c-
    labels:
      k8s-app: kube-dns
      pod-template-hash: ccb96694c
    name: coredns-ccb96694c-b995m
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: coredns-ccb96694c
      uid: 116d85a5-0fd0-41de-96ea-909c3b67a3af
    resourceVersion: "655"
    uid: 02e0a487-d362-46a2-986a-3cb474cba55b
  spec:
    containers:
    - args:
      - -conf
      - /etc/coredns/Corefile
      image: rancher/mirrored-coredns-coredns:1.12.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: coredns
      ports:
      - containerPort: 53
        name: dns
        protocol: UDP
      - containerPort: 53
        name: dns-tcp
        protocol: TCP
      - containerPort: 9153
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8181
          scheme: HTTP
        periodSeconds: 2
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 70Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - all
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
      - mountPath: /etc/coredns/custom
        name: custom-config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wpcrp
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    nodeName: homelab
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: coredns
    serviceAccountName: coredns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchLabels:
          k8s-app: kube-dns
      maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: DoNotSchedule
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: Corefile
          path: Corefile
        - key: NodeHosts
          path: NodeHosts
        name: coredns
      name: config-volume
    - configMap:
        defaultMode: 420
        name: coredns-custom
        optional: true
      name: custom-config-volume
    - name: kube-api-access-wpcrp
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-21T16:47:16Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-21T16:46:29Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-21T16:47:17Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-21T16:47:17Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-21T16:46:29Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://ecba5fd7bd1c330fc509bcf0d8562c5f1c11a8d6cac9635510b90b97c78a387e
      image: docker.io/rancher/mirrored-coredns-coredns:1.12.0
      imageID: docker.io/rancher/mirrored-coredns-coredns@sha256:82979ddf442c593027a57239ad90616deb874e90c365d1a96ad508c2104bdea5
      lastState: {}
      name: coredns
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-21T16:47:14Z"
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/coredns/custom
        name: custom-config-volume
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wpcrp
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.0.10
    hostIPs:
    - ip: 10.0.0.10
    - ip: 2a06:c701:7018:2d00:e2d5:5eff:fe8a:76a4
    phase: Running
    podIP: 10.42.0.4
    podIPs:
    - ip: 10.42.0.4
    qosClass: Burstable
    startTime: "2025-03-21T16:46:29Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      helmcharts.helm.cattle.io/configHash: SHA256=F2E71718982652EFD14100E53A28FED0592F391FEA82F2F946F3845947A2CD68
    creationTimestamp: "2025-03-21T16:46:28Z"
    generateName: helm-install-traefik-
    labels:
      batch.kubernetes.io/controller-uid: 1e5bfd3d-8df4-459b-8f92-2747ec21c2b6
      batch.kubernetes.io/job-name: helm-install-traefik
      controller-uid: 1e5bfd3d-8df4-459b-8f92-2747ec21c2b6
      helmcharts.helm.cattle.io/chart: traefik
      job-name: helm-install-traefik
    name: helm-install-traefik-7r6vh
    namespace: kube-system
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: helm-install-traefik
      uid: 1e5bfd3d-8df4-459b-8f92-2747ec21c2b6
    resourceVersion: "663"
    uid: c9178f22-8c80-4c3c-9582-bb8e3c656b20
  spec:
    containers:
    - args:
      - install
      - --set-string
      - global.systemDefaultRegistry=
      env:
      - name: NAME
        value: traefik
      - name: VERSION
      - name: REPO
      - name: HELM_DRIVER
        value: secret
      - name: CHART_NAMESPACE
        value: kube-system
      - name: CHART
        value: https://%{KUBERNETES_API}%/static/charts/traefik-27.0.201+up27.0.2.tgz
      - name: HELM_VERSION
      - name: TARGET_NAMESPACE
        value: kube-system
      - name: AUTH_PASS_CREDENTIALS
        value: "false"
      - name: INSECURE_SKIP_TLS_VERIFY
        value: "false"
      - name: PLAIN_HTTP
        value: "false"
      - name: NO_PROXY
        value: .svc,.cluster.local,10.42.0.0/16,10.43.0.0/16
      - name: FAILURE_POLICY
        value: reinstall
      image: rancher/klipper-helm:v0.9.4-build20250113
      imagePullPolicy: IfNotPresent
      name: helm
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /home/klipper-helm/.helm
        name: klipper-helm
      - mountPath: /home/klipper-helm/.cache
        name: klipper-cache
      - mountPath: /home/klipper-helm/.config
        name: klipper-config
      - mountPath: /tmp
        name: tmp
      - mountPath: /config
        name: values
      - mountPath: /chart
        name: content
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-99dxj
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: homelab
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: helm-traefik
    serviceAccountName: helm-traefik
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        medium: Memory
      name: klipper-helm
    - emptyDir:
        medium: Memory
      name: klipper-cache
    - emptyDir:
        medium: Memory
      name: klipper-config
    - emptyDir:
        medium: Memory
      name: tmp
    - name: values
      secret:
        defaultMode: 420
        secretName: chart-values-traefik
    - configMap:
        defaultMode: 420
        name: chart-content-traefik
      name: content
    - name: kube-api-access-99dxj
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-21T16:47:18Z"
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-21T16:46:28Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-21T16:47:16Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-21T16:47:16Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-21T16:46:28Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://256553b51bb2422c8d68560b9dc8ed8ff530e822c69afc0b97341ad44532109d
      image: docker.io/rancher/klipper-helm:v0.9.4-build20250113
      imageID: docker.io/rancher/klipper-helm@sha256:d8aba471eb96967a3dfc66ef251c93ee5df8dac908459fbb9ed3e99ce0d5946f
      lastState: {}
      name: helm
      ready: false
      restartCount: 1
      started: false
      state:
        terminated:
          containerID: containerd://256553b51bb2422c8d68560b9dc8ed8ff530e822c69afc0b97341ad44532109d
          exitCode: 0
          finishedAt: "2025-03-21T16:47:12Z"
          message: |
            Installing helm chart
          reason: Completed
          startedAt: "2025-03-21T16:47:11Z"
      volumeMounts:
      - mountPath: /home/klipper-helm/.helm
        name: klipper-helm
      - mountPath: /home/klipper-helm/.cache
        name: klipper-cache
      - mountPath: /home/klipper-helm/.config
        name: klipper-config
      - mountPath: /tmp
        name: tmp
      - mountPath: /config
        name: values
      - mountPath: /chart
        name: content
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-99dxj
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.0.10
    hostIPs:
    - ip: 10.0.0.10
    - ip: 2a06:c701:7018:2d00:e2d5:5eff:fe8a:76a4
    phase: Succeeded
    podIP: 10.42.0.6
    podIPs:
    - ip: 10.42.0.6
    qosClass: BestEffort
    startTime: "2025-03-21T16:46:28Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      helmcharts.helm.cattle.io/configHash: SHA256=E3B0C44298FC1C149AFBF4C8996FB92427AE41E4649B934CA495991B7852B855
    creationTimestamp: "2025-03-21T16:46:28Z"
    generateName: helm-install-traefik-crd-
    labels:
      batch.kubernetes.io/controller-uid: d5a720d7-9a0f-45e6-a759-75c8bb879646
      batch.kubernetes.io/job-name: helm-install-traefik-crd
      controller-uid: d5a720d7-9a0f-45e6-a759-75c8bb879646
      helmcharts.helm.cattle.io/chart: traefik-crd
      job-name: helm-install-traefik-crd
    name: helm-install-traefik-crd-kglxb
    namespace: kube-system
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: helm-install-traefik-crd
      uid: d5a720d7-9a0f-45e6-a759-75c8bb879646
    resourceVersion: "646"
    uid: 83e239b2-31d8-47e7-add6-8128fcda7fe6
  spec:
    containers:
    - args:
      - install
      env:
      - name: NAME
        value: traefik-crd
      - name: VERSION
      - name: REPO
      - name: HELM_DRIVER
        value: secret
      - name: CHART_NAMESPACE
        value: kube-system
      - name: CHART
        value: https://%{KUBERNETES_API}%/static/charts/traefik-crd-27.0.201+up27.0.2.tgz
      - name: HELM_VERSION
      - name: TARGET_NAMESPACE
        value: kube-system
      - name: AUTH_PASS_CREDENTIALS
        value: "false"
      - name: INSECURE_SKIP_TLS_VERIFY
        value: "false"
      - name: PLAIN_HTTP
        value: "false"
      - name: NO_PROXY
        value: .svc,.cluster.local,10.42.0.0/16,10.43.0.0/16
      - name: FAILURE_POLICY
        value: reinstall
      image: rancher/klipper-helm:v0.9.4-build20250113
      imagePullPolicy: IfNotPresent
      name: helm
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /home/klipper-helm/.helm
        name: klipper-helm
      - mountPath: /home/klipper-helm/.cache
        name: klipper-cache
      - mountPath: /home/klipper-helm/.config
        name: klipper-config
      - mountPath: /tmp
        name: tmp
      - mountPath: /config
        name: values
      - mountPath: /chart
        name: content
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t9t8t
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: homelab
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: helm-traefik-crd
    serviceAccountName: helm-traefik-crd
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        medium: Memory
      name: klipper-helm
    - emptyDir:
        medium: Memory
      name: klipper-cache
    - emptyDir:
        medium: Memory
      name: klipper-config
    - emptyDir:
        medium: Memory
      name: tmp
    - name: values
      secret:
        defaultMode: 420
        secretName: chart-values-traefik-crd
    - configMap:
        defaultMode: 420
        name: chart-content-traefik-crd
      name: content
    - name: kube-api-access-t9t8t
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-21T16:47:15Z"
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-21T16:46:28Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-21T16:47:10Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-21T16:47:10Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-21T16:46:28Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://c25f4f4207677d91ed350230a93ed2466456eea234b5f7fbf5062fc62ed20b8a
      image: docker.io/rancher/klipper-helm:v0.9.4-build20250113
      imageID: docker.io/rancher/klipper-helm@sha256:d8aba471eb96967a3dfc66ef251c93ee5df8dac908459fbb9ed3e99ce0d5946f
      lastState: {}
      name: helm
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://c25f4f4207677d91ed350230a93ed2466456eea234b5f7fbf5062fc62ed20b8a
          exitCode: 0
          finishedAt: "2025-03-21T16:47:06Z"
          message: |
            Installing helm chart
          reason: Completed
          startedAt: "2025-03-21T16:47:05Z"
      volumeMounts:
      - mountPath: /home/klipper-helm/.helm
        name: klipper-helm
      - mountPath: /home/klipper-helm/.cache
        name: klipper-cache
      - mountPath: /home/klipper-helm/.config
        name: klipper-config
      - mountPath: /tmp
        name: tmp
      - mountPath: /config
        name: values
      - mountPath: /chart
        name: content
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t9t8t
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.0.10
    hostIPs:
    - ip: 10.0.0.10
    - ip: 2a06:c701:7018:2d00:e2d5:5eff:fe8a:76a4
    phase: Succeeded
    podIP: 10.42.0.5
    podIPs:
    - ip: 10.42.0.5
    qosClass: BestEffort
    startTime: "2025-03-21T16:46:28Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-21T16:46:29Z"
    generateName: local-path-provisioner-5b5f758bcf-
    labels:
      app: local-path-provisioner
      pod-template-hash: 5b5f758bcf
    name: local-path-provisioner-5b5f758bcf-6lxf2
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: local-path-provisioner-5b5f758bcf
      uid: 3172e122-afee-4261-a5dc-67e9ee13e363
    resourceVersion: "493"
    uid: fde8c91e-af12-4e70-9e65-de4113fdc5c6
  spec:
    containers:
    - command:
      - local-path-provisioner
      - start
      - --config
      - /etc/config/config.json
      env:
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: rancher/local-path-provisioner:v0.0.31
      imagePullPolicy: IfNotPresent
      name: local-path-provisioner
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config/
        name: config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6w7qb
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: homelab
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: local-path-provisioner-service-account
    serviceAccountName: local-path-provisioner-service-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: local-path-config
      name: config-volume
    - name: kube-api-access-6w7qb
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-21T16:47:03Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-21T16:46:29Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-21T16:47:03Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-21T16:47:03Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-21T16:46:29Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://b0fd8d8c90aa0ca1adf1896f50866af6d3daacbb7f4dc74dd6eda7c4fb6d0f66
      image: docker.io/rancher/local-path-provisioner:v0.0.31
      imageID: docker.io/rancher/local-path-provisioner@sha256:80496fdeb307541007621959aa13aed41d31db9cd2dc4167c19833e0bfa3878c
      lastState: {}
      name: local-path-provisioner
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-21T16:47:02Z"
      volumeMounts:
      - mountPath: /etc/config/
        name: config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6w7qb
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.0.10
    hostIPs:
    - ip: 10.0.0.10
    - ip: 2a06:c701:7018:2d00:e2d5:5eff:fe8a:76a4
    phase: Running
    podIP: 10.42.0.2
    podIPs:
    - ip: 10.42.0.2
    qosClass: BestEffort
    startTime: "2025-03-21T16:46:29Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-21T16:46:29Z"
    generateName: metrics-server-7bf7d58749-
    labels:
      k8s-app: metrics-server
      pod-template-hash: 7bf7d58749
    name: metrics-server-7bf7d58749-g5vdw
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: metrics-server-7bf7d58749
      uid: 5cb48696-9ee5-4663-b8c6-265500996ac6
    resourceVersion: "631"
    uid: 9b5d2c8d-451c-4906-aa04-43d7e745cf59
  spec:
    containers:
    - args:
      - --cert-dir=/tmp
      - --secure-port=10250
      - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      - --kubelet-use-node-status-port
      - --metric-resolution=15s
      - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305
      image: rancher/mirrored-metrics-server:v0.7.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /livez
          port: https
          scheme: HTTPS
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: metrics-server
      ports:
      - containerPort: 10250
        name: https
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: https
          scheme: HTTPS
        periodSeconds: 2
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 100m
          memory: 70Mi
      securityContext:
        allowPrivilegeEscalation: false
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        runAsUser: 1000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6mrtr
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: homelab
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: metrics-server
    serviceAccountName: metrics-server
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: tmp-dir
    - name: kube-api-access-6mrtr
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-21T16:47:14Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-21T16:46:29Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-21T16:47:15Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-21T16:47:15Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-21T16:46:29Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://0e5f860853a83a45f65ae804bc7088c67edf3537209cfea09939294b407c9622
      image: docker.io/rancher/mirrored-metrics-server:v0.7.2
      imageID: docker.io/rancher/mirrored-metrics-server@sha256:dccf8474fb910fef261d31d9483d7e4c1df7b86cf4d638fb6a7d7c88bd51600a
      lastState: {}
      name: metrics-server
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-21T16:47:12Z"
      volumeMounts:
      - mountPath: /tmp
        name: tmp-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6mrtr
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.0.10
    hostIPs:
    - ip: 10.0.0.10
    - ip: 2a06:c701:7018:2d00:e2d5:5eff:fe8a:76a4
    phase: Running
    podIP: 10.42.0.3
    podIPs:
    - ip: 10.42.0.3
    qosClass: Burstable
    startTime: "2025-03-21T16:46:29Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-21T16:47:12Z"
    generateName: svclb-traefik-a64a8883-
    labels:
      app: svclb-traefik-a64a8883
      controller-revision-hash: 7696874b5b
      pod-template-generation: "1"
      svccontroller.k3s.cattle.io/svcname: traefik
      svccontroller.k3s.cattle.io/svcnamespace: kube-system
    name: svclb-traefik-a64a8883-g6ldq
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: svclb-traefik-a64a8883
      uid: 4254bf3c-565c-4dab-b717-b3426b8c54d0
    resourceVersion: "678"
    uid: 25495cbb-4bcb-4706-b338-8e50ffb6a5b2
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - homelab
    automountServiceAccountToken: false
    containers:
    - env:
      - name: SRC_PORT
        value: "80"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "80"
      - name: DEST_IPS
        value: 10.43.35.209
      image: rancher/klipper-lb:v0.4.10
      imagePullPolicy: IfNotPresent
      name: lb-tcp-80
      ports:
      - containerPort: 80
        hostPort: 80
        name: lb-tcp-80
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    - env:
      - name: SRC_PORT
        value: "443"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "443"
      - name: DEST_IPS
        value: 10.43.35.209
      image: rancher/klipper-lb:v0.4.10
      imagePullPolicy: IfNotPresent
      name: lb-tcp-443
      ports:
      - containerPort: 443
        hostPort: 443
        name: lb-tcp-443
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: homelab
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      sysctls:
      - name: net.ipv4.ip_forward
        value: "1"
    serviceAccount: svclb
    serviceAccountName: svclb
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-21T16:47:26Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-21T16:47:12Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-21T16:47:26Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-21T16:47:26Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-21T16:47:12Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://21ac375555cf9426ef7db171fd86d8fc6f1b516db7efafc04adc02b525532cd2
      image: docker.io/rancher/klipper-lb:v0.4.10
      imageID: docker.io/rancher/klipper-lb@sha256:a3cda7a81a112045d0efd9fc4e3c0e6edc71385c37a93a17eb6217646c4134c6
      lastState: {}
      name: lb-tcp-443
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-21T16:47:25Z"
    - containerID: containerd://40f5e2e7a6c51b341421a8d6c6c53e1abc6fc7ed6ed1ac9784107a7ed67545fd
      image: docker.io/rancher/klipper-lb:v0.4.10
      imageID: docker.io/rancher/klipper-lb@sha256:a3cda7a81a112045d0efd9fc4e3c0e6edc71385c37a93a17eb6217646c4134c6
      lastState: {}
      name: lb-tcp-80
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-21T16:47:24Z"
    hostIP: 10.0.0.10
    hostIPs:
    - ip: 10.0.0.10
    - ip: 2a06:c701:7018:2d00:e2d5:5eff:fe8a:76a4
    phase: Running
    podIP: 10.42.0.7
    podIPs:
    - ip: 10.42.0.7
    qosClass: BestEffort
    startTime: "2025-03-21T16:47:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      prometheus.io/path: /metrics
      prometheus.io/port: "9100"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-03-21T16:47:12Z"
    generateName: traefik-5cbdcf97f4-
    labels:
      app.kubernetes.io/instance: traefik-kube-system
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: traefik
      helm.sh/chart: traefik-27.0.201_up27.0.2
      pod-template-hash: 5cbdcf97f4
    name: traefik-5cbdcf97f4-mhz4p
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: traefik-5cbdcf97f4
      uid: 2ba6cfdb-1967-4622-80b7-549d9386f951
    resourceVersion: "689"
    uid: 2db86def-ba7b-4b34-bc99-f5d9b5a28855
  spec:
    containers:
    - args:
      - --global.checknewversion
      - --global.sendanonymoususage
      - --entrypoints.metrics.address=:9100/tcp
      - --entrypoints.traefik.address=:9000/tcp
      - --entrypoints.web.address=:8000/tcp
      - --entrypoints.websecure.address=:8443/tcp
      - --api.dashboard=true
      - --ping=true
      - --metrics.prometheus=true
      - --metrics.prometheus.entrypoint=metrics
      - --providers.kubernetescrd
      - --providers.kubernetesingress
      - --providers.kubernetesingress.ingressendpoint.publishedservice=kube-system/traefik
      - --entrypoints.websecure.http.tls=true
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: rancher/mirrored-library-traefik:2.11.20
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /ping
          port: 9000
          scheme: HTTP
        initialDelaySeconds: 2
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 2
      name: traefik
      ports:
      - containerPort: 9100
        name: metrics
        protocol: TCP
      - containerPort: 9000
        name: traefik
        protocol: TCP
      - containerPort: 8000
        name: web
        protocol: TCP
      - containerPort: 8443
        name: websecure
        protocol: TCP
      readinessProbe:
        failureThreshold: 1
        httpGet:
          path: /ping
          port: 9000
          scheme: HTTP
        initialDelaySeconds: 2
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 2
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /data
        name: data
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-l4bkm
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: homelab
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroupChangePolicy: OnRootMismatch
      runAsGroup: 65532
      runAsNonRoot: true
      runAsUser: 65532
    serviceAccount: traefik
    serviceAccountName: traefik
    terminationGracePeriodSeconds: 60
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: data
    - emptyDir: {}
      name: tmp
    - name: kube-api-access-l4bkm
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-21T16:47:27Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-21T16:47:12Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-21T16:47:29Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-21T16:47:29Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-21T16:47:12Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://f9ff869ed20f639661f256c764db2867afe0ac677878fc120abed2ed94f9678d
      image: docker.io/rancher/mirrored-library-traefik:2.11.20
      imageID: docker.io/rancher/mirrored-library-traefik@sha256:21f5c16b22154b59cd2bab3b1fb397474307f398cd720152be28bde6ca942380
      lastState: {}
      name: traefik
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-21T16:47:27Z"
      volumeMounts:
      - mountPath: /data
        name: data
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-l4bkm
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.0.10
    hostIPs:
    - ip: 10.0.0.10
    - ip: 2a06:c701:7018:2d00:e2d5:5eff:fe8a:76a4
    phase: Running
    podIP: 10.42.0.8
    podIPs:
    - ip: 10.42.0.8
    qosClass: BestEffort
    startTime: "2025-03-21T16:47:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-22T14:38:01Z"
    generateName: jellyfin-cf758fc6b-
    labels:
      app.kubernetes.io/instance: jellyfin
      app.kubernetes.io/name: jellyfin
      pod-template-hash: cf758fc6b
    name: jellyfin-cf758fc6b-shv2h
    namespace: media
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: jellyfin-cf758fc6b
      uid: e978bd67-6139-427e-a5e5-04a94d616523
    resourceVersion: "26179"
    uid: 5d4a8e54-401d-4167-a045-5186e4b3ddae
  spec:
    automountServiceAccountToken: true
    containers:
    - env:
      - name: TZ
        value: UTC
      image: jellyfin/jellyfin:latest
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: 8096
        timeoutSeconds: 1
      name: jellyfin
      ports:
      - containerPort: 8096
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: 8096
        timeoutSeconds: 1
      resources: {}
      startupProbe:
        failureThreshold: 30
        periodSeconds: 5
        successThreshold: 1
        tcpSocket:
          port: 8096
        timeoutSeconds: 1
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /config
        name: config
      - mountPath: /data/media
        name: media
        subPath: media
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9rnz6
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: homelab
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: config
      persistentVolumeClaim:
        claimName: jellyfin-config
    - name: media
      persistentVolumeClaim:
        claimName: nfs-pvc
    - name: kube-api-access-9rnz6
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-22T14:38:08Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-22T14:38:06Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-22T14:38:17Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-22T14:38:17Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-22T14:38:06Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://86f737a634e0fb1ff675428bb7359d65595660608e736916cc9175419b9c327f
      image: docker.io/jellyfin/jellyfin:latest
      imageID: docker.io/jellyfin/jellyfin@sha256:96b09723b22fdde74283274bdc1f63b9b76768afd6045dd80d4a4559fc4bb7f3
      lastState: {}
      name: jellyfin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-22T14:38:07Z"
      volumeMounts:
      - mountPath: /config
        name: config
      - mountPath: /data/media
        name: media
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9rnz6
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.0.10
    hostIPs:
    - ip: 10.0.0.10
    - ip: 2a06:c701:7018:2d00:e2d5:5eff:fe8a:76a4
    phase: Running
    podIP: 10.42.0.48
    podIPs:
    - ip: 10.42.0.48
    qosClass: BestEffort
    startTime: "2025-03-22T14:38:06Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-22T14:42:54Z"
    generateName: radarr-5d7bb4bd74-
    labels:
      app.kubernetes.io/instance: radarr
      app.kubernetes.io/name: radarr
      pod-template-hash: 5d7bb4bd74
    name: radarr-5d7bb4bd74-rnklh
    namespace: media
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: radarr-5d7bb4bd74
      uid: e39674e2-665c-4f95-b93c-2a75387b9658
    resourceVersion: "26355"
    uid: ec1ebc81-7e6b-49f3-9ce4-c8237d9ae79c
  spec:
    automountServiceAccountToken: true
    containers:
    - env:
      - name: TZ
        value: UTC
      image: linuxserver/radarr:latest
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /usr/bin/env
          - bash
          - -c
          - curl --fail localhost:7878/api/v3/system/status?apiKey=`IFS=\> && while
            read -d \< E C; do if [[ $E = "ApiKey" ]]; then echo $C; fi; done < /config/config.xml`
        failureThreshold: 5
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 10
      name: radarr
      ports:
      - containerPort: 7878
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: 7878
        timeoutSeconds: 1
      resources: {}
      startupProbe:
        failureThreshold: 30
        periodSeconds: 5
        successThreshold: 1
        tcpSocket:
          port: 7878
        timeoutSeconds: 1
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /config
        name: config
      - mountPath: /data
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vzlh8
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: homelab
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: config
      persistentVolumeClaim:
        claimName: radarr-config
    - name: data
      persistentVolumeClaim:
        claimName: nfs-pvc
    - name: kube-api-access-vzlh8
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-22T14:42:56Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-22T14:42:54Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-22T14:43:04Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-22T14:43:04Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-22T14:42:54Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://57567647439375f1990f9a416f220c76e0d1b49cb1a8252b17592c027821fd48
      image: docker.io/linuxserver/radarr:latest
      imageID: docker.io/linuxserver/radarr@sha256:23677e1cb09bd957796f4521748f0eff9eb65d883949c10442635eabe64b750a
      lastState: {}
      name: radarr
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-22T14:42:56Z"
      volumeMounts:
      - mountPath: /config
        name: config
      - mountPath: /data
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vzlh8
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.0.10
    hostIPs:
    - ip: 10.0.0.10
    - ip: 2a06:c701:7018:2d00:e2d5:5eff:fe8a:76a4
    phase: Running
    podIP: 10.42.0.50
    podIPs:
    - ip: 10.42.0.50
    qosClass: BestEffort
    startTime: "2025-03-22T14:42:54Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-22T14:42:50Z"
    generateName: sonarr-6bc9d4f68f-
    labels:
      app.kubernetes.io/instance: sonarr
      app.kubernetes.io/name: sonarr
      pod-template-hash: 6bc9d4f68f
    name: sonarr-6bc9d4f68f-zndxr
    namespace: media
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: sonarr-6bc9d4f68f
      uid: 349bb852-0ff8-4965-9132-dc8379a54f09
    resourceVersion: "26346"
    uid: 6d4dd361-478b-40df-842a-2f6a9f0d9fe5
  spec:
    automountServiceAccountToken: true
    containers:
    - env:
      - name: TZ
        value: UTC
      image: linuxserver/sonarr:latest
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /usr/bin/env
          - bash
          - -c
          - curl --fail localhost:8989/api/v3/system/status?apiKey=`IFS=\> && while
            read -d \< E C; do if [[ $E = "ApiKey" ]]; then echo $C; fi; done < /config/config.xml`
        failureThreshold: 5
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 10
      name: sonarr
      ports:
      - containerPort: 8989
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: 8989
        timeoutSeconds: 1
      resources: {}
      startupProbe:
        failureThreshold: 30
        periodSeconds: 5
        successThreshold: 1
        tcpSocket:
          port: 8989
        timeoutSeconds: 1
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /config
        name: config
      - mountPath: /data
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xh4cc
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: homelab
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: config
      persistentVolumeClaim:
        claimName: sonarr-config
    - name: data
      persistentVolumeClaim:
        claimName: nfs-pvc
    - name: kube-api-access-xh4cc
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-22T14:42:52Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-22T14:42:50Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-22T14:43:01Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-22T14:43:01Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-22T14:42:50Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://f5589deaa3c5549a713a34f0d5728fdfb2a0e31737127c7d3f7cd33b4677f224
      image: docker.io/linuxserver/sonarr:latest
      imageID: docker.io/linuxserver/sonarr@sha256:7fe49f99201de94a277c577dcce5ef8f1789ead1056c8cf758fac7bf4e601d16
      lastState: {}
      name: sonarr
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-22T14:42:51Z"
      volumeMounts:
      - mountPath: /config
        name: config
      - mountPath: /data
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xh4cc
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.0.10
    hostIPs:
    - ip: 10.0.0.10
    - ip: 2a06:c701:7018:2d00:e2d5:5eff:fe8a:76a4
    phase: Running
    podIP: 10.42.0.49
    podIPs:
    - ip: 10.42.0.49
    qosClass: BestEffort
    startTime: "2025-03-22T14:42:50Z"
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-03-21T16:46:22Z"
    labels:
      component: apiserver
      provider: kubernetes
    name: kubernetes
    namespace: default
    resourceVersion: "195"
    uid: 1c53837f-1707-4085-bdc8-8a9a1ead6c0a
  spec:
    clusterIP: 10.43.0.1
    clusterIPs:
    - 10.43.0.1
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: 6443
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/4ySQYvbMBCF/0p5Z9m142TjFfRQdimUQgmk7aXsQZYnG9W2JKRJSgj+70WJl00b0vZm8958vHmjI5Q33yhE4ywk9iUEOmNbSKwp7I0mCAzEqlWsII9Q1jpWbJyN6dc1P0hzJM6DcblWzD3lxr01iQBxU3c/LYXsed9BoqvihbIvxZtPxrbv3rets/9EWDUQJLQL1Nr4X/bolU4z3a6hLB4i0wABH9xAvKVdTG7vAkPivlxUV1rUQfkE4LAjjAK9aqg/1dHVMVPev8DPidJnsMR0mtb9LjKFLE71Tpg/bdNeDy7Q4+f1X/baqriFRKNpVlez+7ouy+W8UkVV36lmURab2eZuSZvlfDYv9GKZ8k7si4i3ahkFoiedVptyf1xBoizyeZUXeVlAvAoR8vul9CRg/Ac1mP6wcr3Rh/SojH3uac1Kd6lXFzhNHV8indOcy19Up+LZaddD4uvjCqO4dGas/S33l4ff3ANxMPqVne567X8SiNSTZhduHHMcx18BAAD//5X9LCMyAwAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: coredns
      objectset.rio.cattle.io/owner-namespace: kube-system
      prometheus.io/port: "9153"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-03-21T16:46:26Z"
    labels:
      k8s-app: kube-dns
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: CoreDNS
      objectset.rio.cattle.io/hash: bce283298811743a0386ab510f2f67ef74240c57
    name: kube-dns
    namespace: kube-system
    resourceVersion: "270"
    uid: 5e781889-2ee6-41f3-b45c-9fa194140fe1
  spec:
    clusterIP: 10.43.0.10
    clusterIPs:
    - 10.43.0.10
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: dns
      port: 53
      protocol: UDP
      targetPort: 53
    - name: dns-tcp
      port: 53
      protocol: TCP
      targetPort: 53
    - name: metrics
      port: 9153
      protocol: TCP
      targetPort: 9153
    selector:
      k8s-app: kube-dns
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/4SQQWsbMRCF/0p5Z9nNep04FfRQWnopBUNKL6WHWe04VleWhGa8xZj970UbFxLaJCchvZn3vqczKPvvXMSnCIuxgcHgYw+LOy6jdwyDAyv1pAR7BsWYlNSnKPWaul/sVFiXxaelI9XAS5/e+uoA86yefkcui/txgMXQyiNlbMybLz727z/0fYqvWkQ6MGxFLN7JQriMXObjgf31bcnkqsVw7HghJ1E+YDII1HGYO1ahRFaWuujCUfRRhIWWY016Onbh+vqE6wWePckeFnTdt527uWrc7abhZtXuqF11q83uev2uu2HabK46t1tTJfxvdTy8P1NKMrtayefPdPDhtE3BuxMstoV3XD4dKdwpuQEGORUV2B/nvzl71SwXAXa9bg1ySZpcCrD49nELA6Vyz7qdJy4L008D4cBOU5l/81YWlPO/4NM0/QkAAP//sKxN444CAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: metrics-server-service
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-03-21T16:46:26Z"
    labels:
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: Metrics-server
      objectset.rio.cattle.io/hash: a5d3bc601c871e123fa32b27f549b6ea770bcf4a
    name: metrics-server
    namespace: kube-system
    resourceVersion: "314"
    uid: c6e50de3-dcf2-45f4-96f1-7445681b8bf3
  spec:
    clusterIP: 10.43.229.244
    clusterIPs:
    - 10.43.229.244
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: PreferDualStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: https
    selector:
      k8s-app: metrics-server
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: traefik
      meta.helm.sh/release-namespace: kube-system
    creationTimestamp: "2025-03-21T16:47:12Z"
    finalizers:
    - service.kubernetes.io/load-balancer-cleanup
    labels:
      app.kubernetes.io/instance: traefik-kube-system
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: traefik
      helm.sh/chart: traefik-27.0.201_up27.0.2
    name: traefik
    namespace: kube-system
    resourceVersion: "680"
    uid: a64a8883-1042-41b8-a40b-b35be6bec3c3
  spec:
    allocateLoadBalancerNodePorts: true
    clusterIP: 10.43.35.209
    clusterIPs:
    - 10.43.35.209
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: PreferDualStack
    ports:
    - name: web
      nodePort: 32057
      port: 80
      protocol: TCP
      targetPort: web
    - name: websecure
      nodePort: 30236
      port: 443
      protocol: TCP
      targetPort: websecure
    selector:
      app.kubernetes.io/instance: traefik-kube-system
      app.kubernetes.io/name: traefik
    sessionAffinity: None
    type: LoadBalancer
  status:
    loadBalancer:
      ingress:
      - ip: 10.0.0.10
        ipMode: VIP
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: jellyfin
      meta.helm.sh/release-namespace: media
    creationTimestamp: "2025-03-22T14:38:01Z"
    labels:
      app.kubernetes.io/instance: jellyfin
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: jellyfin
      app.kubernetes.io/version: 10.8.1
      helm.sh/chart: jellyfin-9.5.3
    name: jellyfin
    namespace: media
    resourceVersion: "26128"
    uid: 50af699b-754c-4c9d-8b80-484f537d3488
  spec:
    clusterIP: 10.43.244.117
    clusterIPs:
    - 10.43.244.117
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8096
      protocol: TCP
      targetPort: http
    selector:
      app.kubernetes.io/instance: jellyfin
      app.kubernetes.io/name: jellyfin
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: radarr
      meta.helm.sh/release-namespace: media
    creationTimestamp: "2025-03-22T14:28:08Z"
    labels:
      app.kubernetes.io/instance: radarr
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: radarr
      app.kubernetes.io/version: v4.1.0.6175
      helm.sh/chart: radarr-16.3.2
    name: radarr
    namespace: media
    resourceVersion: "25722"
    uid: ed24430e-68e8-4118-9b99-8e27c91efd72
  spec:
    clusterIP: 10.43.115.129
    clusterIPs:
    - 10.43.115.129
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 7878
      protocol: TCP
      targetPort: http
    selector:
      app.kubernetes.io/instance: radarr
      app.kubernetes.io/name: radarr
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: sonarr
      meta.helm.sh/release-namespace: media
    creationTimestamp: "2025-03-22T14:28:11Z"
    labels:
      app.kubernetes.io/instance: sonarr
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: sonarr
      app.kubernetes.io/version: v3.0.8.1507
      helm.sh/chart: sonarr-16.3.2
    name: sonarr
    namespace: media
    resourceVersion: "25753"
    uid: 4774b960-90d8-4fa5-a40e-d4b1691a09cb
  spec:
    clusterIP: 10.43.6.13
    clusterIPs:
    - 10.43.6.13
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8989
      protocol: TCP
      targetPort: http
    selector:
      app.kubernetes.io/instance: sonarr
      app.kubernetes.io/name: sonarr
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/8RVwW7jNhD9lWLOsiJHSuoI6CFIgiJo1zFsby8LIxhRo5g1RRLkSBsj0L8XlJ2s0rWToEB34YNBcuZx+N7ozROglX+R89JoyAGt9SftGCLYSF1CDtdItdELYoigJsYSGSF/AtTaMLI02oelKf4mwZ44dtLEApkVxdKcyIAB0dFz81WTGz20G8jhpB1Hv/whdfnbglwrBb2bp7EmyIEdUiU3Hwr3FkXI2TQFjfzWM9XQRSAc9Y9Zypo8Y20h141SESgsSL35xDX6NeQwSc8rUQhKTk/LsxLprEjPzy+yisaEVTq5EBcpVr+WAiLwrRBGszNKkYs3qR+gaVOSJ0WCjYMcKlSe3knxrfiOiA/EH2FiD+VboYrRHnCE5xlOJpMUdudHUr0lEZj6Vv8T1Mhi/ecLiWjtcfCui4CptgqZ+txBv31AoDexfxyFAyKwYVObRvO+oS+FCKul2ZCGvNc2gnAJSk3OQ/7lCUi3/f++nsX86n52N19CBC2qJmxNEuiiVwHzy+nvN4tBSBL3v5NXkdc3i+X9bH63vBtELq9m38e8dV8fcTsb3jZO4iyN07P4NLmAbhWBrPEhHDjUYk3uZKOkteRGqsjbJM7icQL7oFmj1MwoKbaQw201NTxz5EkzvHRiUFPY0STkWON4R9MLazPjGPJJEsHaeP62OpTtDBth1POzVxE48qZxgkIDBeFINE7y9spopkfuGw8tFlJJlrTrsrKE/AtMb5b3l9efbqew6rpAz/u6ZVn6Y4X714U/S7lQxhvSZVk61K5fHgT439RbBXBp+lSF3k/3Fth/0KPgyCPhJEuBCg7e4rdesPJD/TVxLG2bxdLeV8Z9RVcOeYdu1Rc8tIXpwHkhAjaK3POEDcZQVSQYcpiahVhT2agwGDYU+O9rdEZRHKzIaWLywadq9EwuDEYbsPqRcvMoPfu+Mf4L5N4TR1ahpqPIO4yrPWuXZWm0v9NqezhhFUyzsSUyLdgh08M20BqsV+qHz/3Bbpg8ftbYolRYKIJ8HAbG1gbW5q9iexNm5KYXXTTOkeZpUxfknh9aQp5EUJKXjspDR7rf+yS9P7A9Jyy3kCdd908AAAD//4UaLgBBCQAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: /v1, Kind=Service
      objectset.rio.cattle.io/owner-name: traefik
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-03-21T16:47:12Z"
    generation: 1
    labels:
      objectset.rio.cattle.io/hash: 836fcbce022d5dae5b36694fe1eaf389c93af7dc
      svccontroller.k3s.cattle.io/nodeselector: "false"
      svccontroller.k3s.cattle.io/svcname: traefik
      svccontroller.k3s.cattle.io/svcnamespace: kube-system
    name: svclb-traefik-a64a8883
    namespace: kube-system
    resourceVersion: "679"
    uid: 4254bf3c-565c-4dab-b717-b3426b8c54d0
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: svclb-traefik-a64a8883
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: svclb-traefik-a64a8883
          svccontroller.k3s.cattle.io/svcname: traefik
          svccontroller.k3s.cattle.io/svcnamespace: kube-system
      spec:
        automountServiceAccountToken: false
        containers:
        - env:
          - name: SRC_PORT
            value: "80"
          - name: SRC_RANGES
            value: 0.0.0.0/0
          - name: DEST_PROTO
            value: TCP
          - name: DEST_PORT
            value: "80"
          - name: DEST_IPS
            value: 10.43.35.209
          image: rancher/klipper-lb:v0.4.10
          imagePullPolicy: IfNotPresent
          name: lb-tcp-80
          ports:
          - containerPort: 80
            hostPort: 80
            name: lb-tcp-80
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - env:
          - name: SRC_PORT
            value: "443"
          - name: SRC_RANGES
            value: 0.0.0.0/0
          - name: DEST_PROTO
            value: TCP
          - name: DEST_PORT
            value: "443"
          - name: DEST_IPS
            value: 10.43.35.209
          image: rancher/klipper-lb:v0.4.10
          imagePullPolicy: IfNotPresent
          name: lb-tcp-443
          ports:
          - containerPort: 443
            hostPort: 443
            name: lb-tcp-443
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          sysctls:
          - name: net.ipv4.ip_forward
            value: "1"
        serviceAccount: svclb
        serviceAccountName: svclb
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - key: CriticalAddonsOnly
          operator: Exists
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 1
    desiredNumberScheduled: 1
    numberAvailable: 1
    numberMisscheduled: 0
    numberReady: 1
    observedGeneration: 1
    updatedNumberScheduled: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xVQW/bOBP9Kx/mLMVW0jaugO/QjbPboq3XqJNeiqCgqZHFNcXhkiMnRqD/vhhJduw2TdrFniyTb4ZvHucN70F58xlDNOQgB+V9HG0ySGBtXAE5TNFb2tboGBKokVWhWEF+D8o5YsWGXJS/tPwLNUfkk2DoRCtmiyeGRkaSQPLDfbp1GNLVZg05rM/iwc4mS/733rji/2+KgtyzKZyqEXLQFLBw8afg0SstMetmiWncRsYa2gSsWqLtilpPYqq830H6vPIZHDJGyTYce0EBp7PFE8dWKlaQw1Lj6eTs9PVkkmXnL87U+GzySi1fZuPytHx1juX5i9MXY/3yXIh8V9ITpKNHLZQDbozc5VsTmcL2g6kNQz5OIKJFzRQEVCvW1YenymwlJQfFuNp2acla41bXvlCMfYq7a6c2yli1tAh51ibAWy/MPh1hZR1rb3dxBy30pNDtQVGaHCvjMETIv9yDCiv5gFSTKyGBEbIeDSqN5CZKYxFuEjC1WgmjoJyuMIxqE4LA0gG8+82zk+z0ZAxDxLyxdk7W6C3k8K6cEc8Dxt4C1mzQYYzzQMuuoFIZ2wS8qgLGimwB+VkCFbP/A1n2vWK591GFynIFCXgKDPlkPJFL0RV2d/z26mouUhln2Cg7Rau2C9Tkigj5q3ECHoOhYr+USXCjNcZ4cHKWAJsaqeEH4GN9JBR6KffKzjtWL8/26AEZiEmThRyup8LwmZCUtT8Ou7p4NOx1dhBYIwej4yOBNwkEVIX5V5JL5PZB8WyS/azi3wt++gt6B4zUBI1da1txYOxbv6YgLZWdjz8a6IB/Nxj7Xe0b2RqP627QDtAeKVZA3QTD2wtyjHddmcpaup0HszEWV3gZtbLdPIa8VDZiAlp5tTTWsOmpqKIQ28wur77+9m42/bq4/PT53cWlOKUI5GVPWQs3bS/6n85uPxHx78biMGhyDg22CWzINjV+pMYNfVTL53zQ/cCOcNB9rjSrtI+EhxN2OX+cY6SbyFQfpOr+p89kvJHmKVzcO3mKpWqsmNhRgYuDeXg80ilCDta45k7uyAdDnfBWxTjrCfRqpNo2kTGkOhg2WlmQawobo/GN1lLM7FvjMVkMu0fzyz2sUYhdDPHdQxe7EhIgL0jhB5d3RppENMKyRM2Qw4wWusKisVJ5n0aqSgNZPDmuR5wXyKbeKof/aeZaSf2Pp7yRaj1ZWm0XXq7mgpy8KGbXMt30X/zyq1Sru8Uab3vzDQe871gec6soctcvCdxW6K5dVGxiafrnCqY0I94XKmz7PtqPxdKsPiovRAxjfXRduxcm2U2a/YoI2YNmVOBbEiX2qIclOe6bodz+wCjD6HxgcxyX7r1BXtpK2b1HnzJLe9O2bftPAAAA//8kyNqv/AkAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: coredns
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-03-21T16:46:26Z"
    generation: 1
    labels:
      k8s-app: kube-dns
      kubernetes.io/name: CoreDNS
      objectset.rio.cattle.io/hash: bce283298811743a0386ab510f2f67ef74240c57
    name: coredns
    namespace: kube-system
    resourceVersion: "659"
    uid: 6565b9df-75f9-42c8-8954-bbfa085c0e8e
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 0
    selector:
      matchLabels:
        k8s-app: kube-dns
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
      spec:
        containers:
        - args:
          - -conf
          - /etc/coredns/Corefile
          image: rancher/mirrored-coredns-coredns:1.12.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: coredns
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          - containerPort: 9153
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8181
              scheme: HTTP
            periodSeconds: 2
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/coredns
            name: config-volume
            readOnly: true
          - mountPath: /etc/coredns/custom
            name: custom-config-volume
            readOnly: true
        dnsPolicy: Default
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns
        serviceAccountName: coredns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              k8s-app: kube-dns
          maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: DoNotSchedule
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: Corefile
              path: Corefile
            - key: NodeHosts
              path: NodeHosts
            name: coredns
          name: config-volume
        - configMap:
            defaultMode: 420
            name: coredns-custom
            optional: true
          name: custom-config-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-03-21T16:46:28Z"
      lastUpdateTime: "2025-03-21T16:46:28Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-03-21T16:46:28Z"
      lastUpdateTime: "2025-03-21T16:47:17Z"
      message: ReplicaSet "coredns-ccb96694c" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xUUW/iOBD+K6d5TgIcLUKR7gG1Pd1pW4padV8qtBqcCbg4tmUP2UYo/301BFqqLe2utE+J7ZnP38z3jbeAXn+lELWzkAN6H3v1ABJYa1tADpfkjWsqsgwJVMRYICPkW0BrHSNrZ6Ms3eKJFEfiLGiXKWQ2lGnX0wICyclz991SSJf1GnJYD+PRST1I/vqibfHPpCic/RTCYkWQg3EKTRrZBVzSLyVFj0oy15sFpbGJTBW0CRhckPmwtBXGFeQwGA/L4bkanZflQg37o7NRf1ieDcvB+bhfjNVojH8XuCjOBPQNSY+8Sn1wtZbmU4Du/ASf6EkJm0Bd/H9aimyudaUZ8n4CkQwpdkGCKmS1un6pAL0/fWsr4ByQadnsLnDGaLt88AUydWDPDxZr1AYXhiAftAlw44Xj3ZtY2afKm0PekVvMb3DZF6qcZdSWQoT8UZZVhWLJx9Pti4xBfJqmytlSLyGBHrHqdav9J3uKzsI8AbL1Dnkvyuz28tt0cnN1P5tcXEECNZoN/RtcJWRKTaa4o/Llf4Ys4h9qzF6Va9t2noCuxH85BLRqRaH3Pue87mf9bCjztkuYbYyZOaNVAzn8X04dzwLFbvg+807tzKaiG7ex3HWskt89z+M2vGJ1G2mXCe1ciFtX0P2RlcSGwRJT3I1NFArabp5FbR+0C5qbC4MxTjvMzrGpwKQqaNYKjUhDodaKJkoJq+lHtaT72BS7YEiAnaFweGwet7AmadDFHn73QMRbaxoZeC+Rwh2unnXkCG2yBSpLUgw5TN29WlGxMfI4dDA7qsEZyt7WKgYMzqTeoKU/ilxh5J1m70DOD0oebC8S3aAXLX62wN7n7WlJ27b9EQAA//8eX0Hn5AUAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: local-storage
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-03-21T16:46:26Z"
    generation: 1
    labels:
      objectset.rio.cattle.io/hash: 183f35c65ffbc3064603f43f1580d8c68a2dabd4
    name: local-path-provisioner
    namespace: kube-system
    resourceVersion: "495"
    uid: 82434a52-398e-4f6e-a7bc-c53d2f301078
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 0
    selector:
      matchLabels:
        app: local-path-provisioner
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: local-path-provisioner
      spec:
        containers:
        - command:
          - local-path-provisioner
          - start
          - --config
          - /etc/config/config.json
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rancher/local-path-provisioner:v0.0.31
          imagePullPolicy: IfNotPresent
          name: local-path-provisioner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config/
            name: config-volume
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: local-path-provisioner-service-account
        serviceAccountName: local-path-provisioner-service-account
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: local-path-config
          name: config-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-03-21T16:46:28Z"
      lastUpdateTime: "2025-03-21T16:46:28Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-03-21T16:46:28Z"
      lastUpdateTime: "2025-03-21T16:47:03Z"
      message: ReplicaSet "local-path-provisioner-5b5f758bcf" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xV4W8auRP9V36az7vJLoHQrsQHRPiV6tIUFXqnUxUh4x3Ah9f2eWZpuIj//TS7SUoupe1V9wUt9vj5zXvjmXtQwfyKkYx3UIAKgc53OSSwNa6EAq4wWL+v0DEkUCGrUrGC4h6Uc54VG+9I/vrlH6iZkM+i8WdaMVs8M/7cCAgkJ/f9Z4cxXe+2UMD2go52dnnyv1+MKwfDsvTuuxBOVQiFUIxGU0oYdxjT8pj+9wEoKC0o23qJKe2JsYJDAlYt0TZpbl9RqkJ4cdE30DeKNlAA5hl2uj3ML7B7qfqdnn5dlmXZXb3GZefi9Srv9vv97qqU+76aC7TrJyhSQC0EI+6MeDkxxD7ur01lGIosAUKLmn2UoEqx3lx/P6mDAHNUjOt9A+6tNW79MZSKsQW6++jUThmrlhahyA8J8D4Ivw/PYmUdq2Afzx0V0g+Ie1KSo8S1d6yMw0hQfLoHFdfyAWmqMXJamjg45ypAAmlKqOuIafCRB3nW6WXNqghqkdMQcYUxYpmqsoxIlEpGNHjrGKNT9u00Gd89fU48ccPtGKImTJ0vMSVWXFNzUxPQ0k8jkre1vJ1B3qNmhy2l2oQNxpRqw0iD+fVsMR5dTcbyOxsufns7nyyG49mi07tcvBm9W8wmw4tX3eRL3IcfivoHWt559RjX6V2eQjsZdYQ2mgxHk2EnW0zfX/+eX2S9r4G9CILbBEyl1uJuVE5vMJ5XJkYvDjy3u9hlZ/2zDiRgzQ4dEk2jXzYFtVLG1hHnm4i08baE4iKBDXN4gyz7QbE8wnM5+Bck0DhSNBGiP+kNNvU1mc+nMykr4wwbZa/Qqv0MtXclQXGZJRAwGl8+LeXytGqtkejo8jwBNhX6mr8EfuNdC5u2bJ+qeNoQbKrz6dwj2xA9e+0tFDAfTeFwm0BEVZqfUkRO7n9ekpeKdP6FIPIQ6qiR2tb1Z43EzbcONRSQZ1nVjJ3Kxz0U0M/embYpyQs2vB95x3jX5KOs9Z+n0eyMxTWOSSvbTCcoVsoSthK9d3b/wXv+v7H40DsLjrXs1m5IN97J7rO1j4RRjMiyQwI7b+sK3/naPfhVyef0Qcq2vzyYxVWQrgOHW/FHusHsqANLp4gOGakZQAQFWOPqO9E5ROOb5KwiumnRWrJtU9HRsNHKikkYd0bjUGvhcXOivNhbjI+j+tM9bFHEHD3ANOOVRBkZYkEihSOM74yYcUjuAVcr1FIcN36mN1jWVvpdC9NQit7i2fOcpJKjt2mwyuF/ilwp4nbivoS8ffSozRSrwPsrI5IfvubM4XD4OwAA///UraDKAQkAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: metrics-server-deployment
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-03-21T16:46:26Z"
    generation: 1
    labels:
      k8s-app: metrics-server
      objectset.rio.cattle.io/hash: e10e245e13e46a725c9dddd4f9eb239f147774fd
    name: metrics-server
    namespace: kube-system
    resourceVersion: "635"
    uid: 8eaccdc0-6866-40a7-ba60-f78e7b279ba4
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 0
    selector:
      matchLabels:
        k8s-app: metrics-server
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: metrics-server
        name: metrics-server
      spec:
        containers:
        - args:
          - --cert-dir=/tmp
          - --secure-port=10250
          - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
          - --kubelet-use-node-status-port
          - --metric-resolution=15s
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305
          image: rancher/mirrored-metrics-server:v0.7.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: https
              scheme: HTTPS
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: metrics-server
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: https
              scheme: HTTPS
            periodSeconds: 2
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp-dir
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: metrics-server
        serviceAccountName: metrics-server
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - emptyDir: {}
          name: tmp-dir
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-03-21T16:46:28Z"
      lastUpdateTime: "2025-03-21T16:46:28Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-03-21T16:46:28Z"
      lastUpdateTime: "2025-03-21T16:47:15Z"
      message: ReplicaSet "metrics-server-7bf7d58749" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: traefik
      meta.helm.sh/release-namespace: kube-system
    creationTimestamp: "2025-03-21T16:47:12Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: traefik-kube-system
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: traefik
      helm.sh/chart: traefik-27.0.201_up27.0.2
    name: traefik
    namespace: kube-system
    resourceVersion: "693"
    uid: 4e8a0c3d-6f1a-4493-b2a8-a2383f77b2ff
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: traefik-kube-system
        app.kubernetes.io/name: traefik
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "9100"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: traefik-kube-system
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: traefik
          helm.sh/chart: traefik-27.0.201_up27.0.2
      spec:
        containers:
        - args:
          - --global.checknewversion
          - --global.sendanonymoususage
          - --entrypoints.metrics.address=:9100/tcp
          - --entrypoints.traefik.address=:9000/tcp
          - --entrypoints.web.address=:8000/tcp
          - --entrypoints.websecure.address=:8443/tcp
          - --api.dashboard=true
          - --ping=true
          - --metrics.prometheus=true
          - --metrics.prometheus.entrypoint=metrics
          - --providers.kubernetescrd
          - --providers.kubernetesingress
          - --providers.kubernetesingress.ingressendpoint.publishedservice=kube-system/traefik
          - --entrypoints.websecure.http.tls=true
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rancher/mirrored-library-traefik:2.11.20
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /ping
              port: 9000
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          name: traefik
          ports:
          - containerPort: 9100
            name: metrics
            protocol: TCP
          - containerPort: 9000
            name: traefik
            protocol: TCP
          - containerPort: 8000
            name: web
            protocol: TCP
          - containerPort: 8443
            name: websecure
            protocol: TCP
          readinessProbe:
            failureThreshold: 1
            httpGet:
              path: /ping
              port: 9000
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: data
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroupChangePolicy: OnRootMismatch
          runAsGroup: 65532
          runAsNonRoot: true
          runAsUser: 65532
        serviceAccount: traefik
        serviceAccountName: traefik
        terminationGracePeriodSeconds: 60
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - emptyDir: {}
          name: data
        - emptyDir: {}
          name: tmp
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-03-21T16:47:29Z"
      lastUpdateTime: "2025-03-21T16:47:29Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-03-21T16:47:12Z"
      lastUpdateTime: "2025-03-21T16:47:29Z"
      message: ReplicaSet "traefik-5cbdcf97f4" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: jellyfin
      meta.helm.sh/release-namespace: media
    creationTimestamp: "2025-03-22T14:38:01Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: jellyfin
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: jellyfin
      app.kubernetes.io/version: 10.8.1
      helm.sh/chart: jellyfin-9.5.3
    name: jellyfin
    namespace: media
    resourceVersion: "26183"
    uid: e045fbb7-48c2-455f-9312-99f2235d7c93
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 3
    selector:
      matchLabels:
        app.kubernetes.io/instance: jellyfin
        app.kubernetes.io/name: jellyfin
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: jellyfin
          app.kubernetes.io/name: jellyfin
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: TZ
            value: UTC
          image: jellyfin/jellyfin:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 8096
            timeoutSeconds: 1
          name: jellyfin
          ports:
          - containerPort: 8096
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 8096
            timeoutSeconds: 1
          resources: {}
          startupProbe:
            failureThreshold: 30
            periodSeconds: 5
            successThreshold: 1
            tcpSocket:
              port: 8096
            timeoutSeconds: 1
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /config
            name: config
          - mountPath: /data/media
            name: media
            subPath: media
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        volumes:
        - name: config
          persistentVolumeClaim:
            claimName: jellyfin-config
        - name: media
          persistentVolumeClaim:
            claimName: nfs-pvc
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-03-22T14:38:17Z"
      lastUpdateTime: "2025-03-22T14:38:17Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-03-22T14:38:01Z"
      lastUpdateTime: "2025-03-22T14:38:17Z"
      message: ReplicaSet "jellyfin-cf758fc6b" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: radarr
      meta.helm.sh/release-namespace: media
    creationTimestamp: "2025-03-22T14:28:08Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: radarr
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: radarr
      app.kubernetes.io/version: v4.1.0.6175
      helm.sh/chart: radarr-16.3.2
    name: radarr
    namespace: media
    resourceVersion: "26359"
    uid: fd9770d4-a372-4845-9085-8cb5fb0803a5
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 3
    selector:
      matchLabels:
        app.kubernetes.io/instance: radarr
        app.kubernetes.io/name: radarr
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: radarr
          app.kubernetes.io/name: radarr
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: TZ
            value: UTC
          image: linuxserver/radarr:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /usr/bin/env
              - bash
              - -c
              - curl --fail localhost:7878/api/v3/system/status?apiKey=`IFS=\> &&
                while read -d \< E C; do if [[ $E = "ApiKey" ]]; then echo $C; fi;
                done < /config/config.xml`
            failureThreshold: 5
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          name: radarr
          ports:
          - containerPort: 7878
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 7878
            timeoutSeconds: 1
          resources: {}
          startupProbe:
            failureThreshold: 30
            periodSeconds: 5
            successThreshold: 1
            tcpSocket:
              port: 7878
            timeoutSeconds: 1
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /config
            name: config
          - mountPath: /data
            name: data
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        volumes:
        - name: config
          persistentVolumeClaim:
            claimName: radarr-config
        - name: data
          persistentVolumeClaim:
            claimName: nfs-pvc
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-03-22T14:43:04Z"
      lastUpdateTime: "2025-03-22T14:43:04Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-03-22T14:28:08Z"
      lastUpdateTime: "2025-03-22T14:43:04Z"
      message: ReplicaSet "radarr-5d7bb4bd74" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: sonarr
      meta.helm.sh/release-namespace: media
    creationTimestamp: "2025-03-22T14:28:11Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: sonarr
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: sonarr
      app.kubernetes.io/version: v3.0.8.1507
      helm.sh/chart: sonarr-16.3.2
    name: sonarr
    namespace: media
    resourceVersion: "26350"
    uid: 63ab095e-0d8a-4633-b823-c73d0188d5c6
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 3
    selector:
      matchLabels:
        app.kubernetes.io/instance: sonarr
        app.kubernetes.io/name: sonarr
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: sonarr
          app.kubernetes.io/name: sonarr
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: TZ
            value: UTC
          image: linuxserver/sonarr:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /usr/bin/env
              - bash
              - -c
              - curl --fail localhost:8989/api/v3/system/status?apiKey=`IFS=\> &&
                while read -d \< E C; do if [[ $E = "ApiKey" ]]; then echo $C; fi;
                done < /config/config.xml`
            failureThreshold: 5
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          name: sonarr
          ports:
          - containerPort: 8989
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 8989
            timeoutSeconds: 1
          resources: {}
          startupProbe:
            failureThreshold: 30
            periodSeconds: 5
            successThreshold: 1
            tcpSocket:
              port: 8989
            timeoutSeconds: 1
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /config
            name: config
          - mountPath: /data
            name: data
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        volumes:
        - name: config
          persistentVolumeClaim:
            claimName: sonarr-config
        - name: data
          persistentVolumeClaim:
            claimName: nfs-pvc
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-03-22T14:43:01Z"
      lastUpdateTime: "2025-03-22T14:43:01Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-03-22T14:28:11Z"
      lastUpdateTime: "2025-03-22T14:43:01Z"
      message: ReplicaSet "sonarr-6bc9d4f68f" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xVQW/bOBP9Kx/mLMVW0jaugO/QjbPboq3XqJNeiqCgqZHFNcXhkiMnRqD/vhhJduw2TdrFniyTb4ZvHucN70F58xlDNOQgB+V9HG0ySGBtXAE5TNFb2tboGBKokVWhWEF+D8o5YsWGXJS/tPwLNUfkk2DoRCtmiyeGRkaSQPLDfbp1GNLVZg05rM/iwc4mS/733rji/2+KgtyzKZyqEXLQFLBw8afg0SstMetmiWncRsYa2gSsWqLtilpPYqq830H6vPIZHDJGyTYce0EBp7PFE8dWKlaQw1Lj6eTs9PVkkmXnL87U+GzySi1fZuPytHx1juX5i9MXY/3yXIh8V9ITpKNHLZQDbozc5VsTmcL2g6kNQz5OIKJFzRQEVCvW1YenymwlJQfFuNp2acla41bXvlCMfYq7a6c2yli1tAh51ibAWy/MPh1hZR1rb3dxBy30pNDtQVGaHCvjMETIv9yDCiv5gFSTKyGBEbIeDSqN5CZKYxFuEjC1WgmjoJyuMIxqE4LA0gG8+82zk+z0ZAxDxLyxdk7W6C3k8K6cEc8Dxt4C1mzQYYzzQMuuoFIZ2wS8qgLGimwB+VkCFbP/A1n2vWK591GFynIFCXgKDPlkPJFL0RV2d/z26mouUhln2Cg7Rau2C9Tkigj5q3ECHoOhYr+USXCjNcZ4cHKWAJsaqeEH4GN9JBR6KffKzjtWL8/26AEZiEmThRyup8LwmZCUtT8Ou7p4NOx1dhBYIwej4yOBNwkEVIX5V5JL5PZB8WyS/azi3wt++gt6B4zUBI1da1txYOxbv6YgLZWdjz8a6IB/Nxj7Xe0b2RqP627QDtAeKVZA3QTD2wtyjHddmcpaup0HszEWV3gZtbLdPIa8VDZiAlp5tTTWsOmpqKIQ28wur77+9m42/bq4/PT53cWlOKUI5GVPWQs3bS/6n85uPxHx78biMGhyDg22CWzINjV+pMYNfVTL53zQ/cCOcNB9rjSrtI+EhxN2OX+cY6SbyFQfpOr+p89kvJHmKVzcO3mKpWqsmNhRgYuDeXg80ilCDta45k7uyAdDnfBWxTjrCfRqpNo2kTGkOhg2WlmQawobo/GN1lLM7FvjMVkMu0fzyz2sUYhdDPHdQxe7EhIgL0jhB5d3RppENMKyRM2Qw4wWusKisVJ5n0aqSgNZPDmuR5wXyKbeKof/aeZaSf2Pp7yRaj1ZWm0XXq7mgpy8KGbXMt30X/zyq1Sru8Uab3vzDQe871gec6soctcvCdxW6K5dVGxiafrnCqY0I94XKmz7PtqPxdKsPiovRAxjfXRduxcm2U2a/YoI2YNmVOBbEiX2qIclOe6bodz+wCjD6HxgcxyX7r1BXtpK2b1HnzJLe9O2bftPAAAA//8kyNqv/AkAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: coredns
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-03-21T16:46:28Z"
    generation: 1
    labels:
      k8s-app: kube-dns
      pod-template-hash: ccb96694c
    name: coredns-ccb96694c
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: coredns
      uid: 6565b9df-75f9-42c8-8954-bbfa085c0e8e
    resourceVersion: "658"
    uid: 116d85a5-0fd0-41de-96ea-909c3b67a3af
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: kube-dns
        pod-template-hash: ccb96694c
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
          pod-template-hash: ccb96694c
      spec:
        containers:
        - args:
          - -conf
          - /etc/coredns/Corefile
          image: rancher/mirrored-coredns-coredns:1.12.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: coredns
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          - containerPort: 9153
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8181
              scheme: HTTP
            periodSeconds: 2
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/coredns
            name: config-volume
            readOnly: true
          - mountPath: /etc/coredns/custom
            name: custom-config-volume
            readOnly: true
        dnsPolicy: Default
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns
        serviceAccountName: coredns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              k8s-app: kube-dns
          maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: DoNotSchedule
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: Corefile
              path: Corefile
            - key: NodeHosts
              path: NodeHosts
            name: coredns
          name: config-volume
        - configMap:
            defaultMode: 420
            name: coredns-custom
            optional: true
          name: custom-config-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xUUW/iOBD+K6d5TgIcLUKR7gG1Pd1pW4padV8qtBqcCbg4tmUP2UYo/301BFqqLe2utE+J7ZnP38z3jbeAXn+lELWzkAN6H3v1ABJYa1tADpfkjWsqsgwJVMRYICPkW0BrHSNrZ6Ms3eKJFEfiLGiXKWQ2lGnX0wICyclz991SSJf1GnJYD+PRST1I/vqibfHPpCic/RTCYkWQg3EKTRrZBVzSLyVFj0oy15sFpbGJTBW0CRhckPmwtBXGFeQwGA/L4bkanZflQg37o7NRf1ieDcvB+bhfjNVojH8XuCjOBPQNSY+8Sn1wtZbmU4Du/ASf6EkJm0Bd/H9aimyudaUZ8n4CkQwpdkGCKmS1un6pAL0/fWsr4ByQadnsLnDGaLt88AUydWDPDxZr1AYXhiAftAlw44Xj3ZtY2afKm0PekVvMb3DZF6qcZdSWQoT8UZZVhWLJx9Pti4xBfJqmytlSLyGBHrHqdav9J3uKzsI8AbL1Dnkvyuz28tt0cnN1P5tcXEECNZoN/RtcJWRKTaa4o/Llf4Ys4h9qzF6Va9t2noCuxH85BLRqRaH3Pue87mf9bCjztkuYbYyZOaNVAzn8X04dzwLFbvg+807tzKaiG7ex3HWskt89z+M2vGJ1G2mXCe1ciFtX0P2RlcSGwRJT3I1NFArabp5FbR+0C5qbC4MxTjvMzrGpwKQqaNYKjUhDodaKJkoJq+lHtaT72BS7YEiAnaFweGwet7AmadDFHn73QMRbaxoZeC+Rwh2unnXkCG2yBSpLUgw5TN29WlGxMfI4dDA7qsEZyt7WKgYMzqTeoKU/ilxh5J1m70DOD0oebC8S3aAXLX62wN7n7WlJ27b9EQAA//8eX0Hn5AUAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: local-storage
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-03-21T16:46:28Z"
    generation: 1
    labels:
      app: local-path-provisioner
      pod-template-hash: 5b5f758bcf
    name: local-path-provisioner-5b5f758bcf
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: local-path-provisioner
      uid: 82434a52-398e-4f6e-a7bc-c53d2f301078
    resourceVersion: "494"
    uid: 3172e122-afee-4261-a5dc-67e9ee13e363
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: local-path-provisioner
        pod-template-hash: 5b5f758bcf
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: local-path-provisioner
          pod-template-hash: 5b5f758bcf
      spec:
        containers:
        - command:
          - local-path-provisioner
          - start
          - --config
          - /etc/config/config.json
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rancher/local-path-provisioner:v0.0.31
          imagePullPolicy: IfNotPresent
          name: local-path-provisioner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config/
            name: config-volume
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: local-path-provisioner-service-account
        serviceAccountName: local-path-provisioner-service-account
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: local-path-config
          name: config-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xV4W8auRP9V36az7vJLoHQrsQHRPiV6tIUFXqnUxUh4x3Ah9f2eWZpuIj//TS7SUoupe1V9wUt9vj5zXvjmXtQwfyKkYx3UIAKgc53OSSwNa6EAq4wWL+v0DEkUCGrUrGC4h6Uc54VG+9I/vrlH6iZkM+i8WdaMVs8M/7cCAgkJ/f9Z4cxXe+2UMD2go52dnnyv1+MKwfDsvTuuxBOVQiFUIxGU0oYdxjT8pj+9wEoKC0o23qJKe2JsYJDAlYt0TZpbl9RqkJ4cdE30DeKNlAA5hl2uj3ML7B7qfqdnn5dlmXZXb3GZefi9Srv9vv97qqU+76aC7TrJyhSQC0EI+6MeDkxxD7ur01lGIosAUKLmn2UoEqx3lx/P6mDAHNUjOt9A+6tNW79MZSKsQW6++jUThmrlhahyA8J8D4Ivw/PYmUdq2Afzx0V0g+Ie1KSo8S1d6yMw0hQfLoHFdfyAWmqMXJamjg45ypAAmlKqOuIafCRB3nW6WXNqghqkdMQcYUxYpmqsoxIlEpGNHjrGKNT9u00Gd89fU48ccPtGKImTJ0vMSVWXFNzUxPQ0k8jkre1vJ1B3qNmhy2l2oQNxpRqw0iD+fVsMR5dTcbyOxsufns7nyyG49mi07tcvBm9W8wmw4tX3eRL3IcfivoHWt559RjX6V2eQjsZdYQ2mgxHk2EnW0zfX/+eX2S9r4G9CILbBEyl1uJuVE5vMJ5XJkYvDjy3u9hlZ/2zDiRgzQ4dEk2jXzYFtVLG1hHnm4i08baE4iKBDXN4gyz7QbE8wnM5+Bck0DhSNBGiP+kNNvU1mc+nMykr4wwbZa/Qqv0MtXclQXGZJRAwGl8+LeXytGqtkejo8jwBNhX6mr8EfuNdC5u2bJ+qeNoQbKrz6dwj2xA9e+0tFDAfTeFwm0BEVZqfUkRO7n9ekpeKdP6FIPIQ6qiR2tb1Z43EzbcONRSQZ1nVjJ3Kxz0U0M/embYpyQs2vB95x3jX5KOs9Z+n0eyMxTWOSSvbTCcoVsoSthK9d3b/wXv+v7H40DsLjrXs1m5IN97J7rO1j4RRjMiyQwI7b+sK3/naPfhVyef0Qcq2vzyYxVWQrgOHW/FHusHsqANLp4gOGakZQAQFWOPqO9E5ROOb5KwiumnRWrJtU9HRsNHKikkYd0bjUGvhcXOivNhbjI+j+tM9bFHEHD3ANOOVRBkZYkEihSOM74yYcUjuAVcr1FIcN36mN1jWVvpdC9NQit7i2fOcpJKjt2mwyuF/ilwp4nbivoS8ffSozRSrwPsrI5IfvubM4XD4OwAA///UraDKAQkAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: metrics-server-deployment
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-03-21T16:46:28Z"
    generation: 1
    labels:
      k8s-app: metrics-server
      pod-template-hash: 7bf7d58749
    name: metrics-server-7bf7d58749
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: metrics-server
      uid: 8eaccdc0-6866-40a7-ba60-f78e7b279ba4
    resourceVersion: "634"
    uid: 5cb48696-9ee5-4663-b8c6-265500996ac6
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: metrics-server
        pod-template-hash: 7bf7d58749
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: metrics-server
          pod-template-hash: 7bf7d58749
        name: metrics-server
      spec:
        containers:
        - args:
          - --cert-dir=/tmp
          - --secure-port=10250
          - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
          - --kubelet-use-node-status-port
          - --metric-resolution=15s
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305
          image: rancher/mirrored-metrics-server:v0.7.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: https
              scheme: HTTPS
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: metrics-server
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: https
              scheme: HTTPS
            periodSeconds: 2
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp-dir
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: metrics-server
        serviceAccountName: metrics-server
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - emptyDir: {}
          name: tmp-dir
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: traefik
      meta.helm.sh/release-namespace: kube-system
    creationTimestamp: "2025-03-21T16:47:12Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: traefik-kube-system
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: traefik
      helm.sh/chart: traefik-27.0.201_up27.0.2
      pod-template-hash: 5cbdcf97f4
    name: traefik-5cbdcf97f4
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: traefik
      uid: 4e8a0c3d-6f1a-4493-b2a8-a2383f77b2ff
    resourceVersion: "692"
    uid: 2ba6cfdb-1967-4622-80b7-549d9386f951
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: traefik-kube-system
        app.kubernetes.io/name: traefik
        pod-template-hash: 5cbdcf97f4
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "9100"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: traefik-kube-system
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: traefik
          helm.sh/chart: traefik-27.0.201_up27.0.2
          pod-template-hash: 5cbdcf97f4
      spec:
        containers:
        - args:
          - --global.checknewversion
          - --global.sendanonymoususage
          - --entrypoints.metrics.address=:9100/tcp
          - --entrypoints.traefik.address=:9000/tcp
          - --entrypoints.web.address=:8000/tcp
          - --entrypoints.websecure.address=:8443/tcp
          - --api.dashboard=true
          - --ping=true
          - --metrics.prometheus=true
          - --metrics.prometheus.entrypoint=metrics
          - --providers.kubernetescrd
          - --providers.kubernetesingress
          - --providers.kubernetesingress.ingressendpoint.publishedservice=kube-system/traefik
          - --entrypoints.websecure.http.tls=true
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rancher/mirrored-library-traefik:2.11.20
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /ping
              port: 9000
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          name: traefik
          ports:
          - containerPort: 9100
            name: metrics
            protocol: TCP
          - containerPort: 9000
            name: traefik
            protocol: TCP
          - containerPort: 8000
            name: web
            protocol: TCP
          - containerPort: 8443
            name: websecure
            protocol: TCP
          readinessProbe:
            failureThreshold: 1
            httpGet:
              path: /ping
              port: 9000
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: data
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroupChangePolicy: OnRootMismatch
          runAsGroup: 65532
          runAsNonRoot: true
          runAsUser: 65532
        serviceAccount: traefik
        serviceAccountName: traefik
        terminationGracePeriodSeconds: 60
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - emptyDir: {}
          name: data
        - emptyDir: {}
          name: tmp
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: jellyfin
      meta.helm.sh/release-namespace: media
    creationTimestamp: "2025-03-22T14:38:01Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: jellyfin
      app.kubernetes.io/name: jellyfin
      pod-template-hash: cf758fc6b
    name: jellyfin-cf758fc6b
    namespace: media
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: jellyfin
      uid: e045fbb7-48c2-455f-9312-99f2235d7c93
    resourceVersion: "26182"
    uid: e978bd67-6139-427e-a5e5-04a94d616523
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: jellyfin
        app.kubernetes.io/name: jellyfin
        pod-template-hash: cf758fc6b
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: jellyfin
          app.kubernetes.io/name: jellyfin
          pod-template-hash: cf758fc6b
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: TZ
            value: UTC
          image: jellyfin/jellyfin:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 8096
            timeoutSeconds: 1
          name: jellyfin
          ports:
          - containerPort: 8096
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 8096
            timeoutSeconds: 1
          resources: {}
          startupProbe:
            failureThreshold: 30
            periodSeconds: 5
            successThreshold: 1
            tcpSocket:
              port: 8096
            timeoutSeconds: 1
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /config
            name: config
          - mountPath: /data/media
            name: media
            subPath: media
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        volumes:
        - name: config
          persistentVolumeClaim:
            claimName: jellyfin-config
        - name: media
          persistentVolumeClaim:
            claimName: nfs-pvc
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: radarr
      meta.helm.sh/release-namespace: media
    creationTimestamp: "2025-03-22T14:42:54Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: radarr
      app.kubernetes.io/name: radarr
      pod-template-hash: 5d7bb4bd74
    name: radarr-5d7bb4bd74
    namespace: media
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: radarr
      uid: fd9770d4-a372-4845-9085-8cb5fb0803a5
    resourceVersion: "26358"
    uid: e39674e2-665c-4f95-b93c-2a75387b9658
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: radarr
        app.kubernetes.io/name: radarr
        pod-template-hash: 5d7bb4bd74
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: radarr
          app.kubernetes.io/name: radarr
          pod-template-hash: 5d7bb4bd74
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: TZ
            value: UTC
          image: linuxserver/radarr:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /usr/bin/env
              - bash
              - -c
              - curl --fail localhost:7878/api/v3/system/status?apiKey=`IFS=\> &&
                while read -d \< E C; do if [[ $E = "ApiKey" ]]; then echo $C; fi;
                done < /config/config.xml`
            failureThreshold: 5
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          name: radarr
          ports:
          - containerPort: 7878
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 7878
            timeoutSeconds: 1
          resources: {}
          startupProbe:
            failureThreshold: 30
            periodSeconds: 5
            successThreshold: 1
            tcpSocket:
              port: 7878
            timeoutSeconds: 1
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /config
            name: config
          - mountPath: /data
            name: data
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        volumes:
        - name: config
          persistentVolumeClaim:
            claimName: radarr-config
        - name: data
          persistentVolumeClaim:
            claimName: nfs-pvc
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: radarr
      meta.helm.sh/release-namespace: media
    creationTimestamp: "2025-03-22T14:28:08Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: radarr
      app.kubernetes.io/name: radarr
      pod-template-hash: "7469466749"
    name: radarr-7469466749
    namespace: media
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: radarr
      uid: fd9770d4-a372-4845-9085-8cb5fb0803a5
    resourceVersion: "26291"
    uid: c45d3332-f03e-4544-a422-94f3600a0888
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: radarr
        app.kubernetes.io/name: radarr
        pod-template-hash: "7469466749"
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: radarr
          app.kubernetes.io/name: radarr
          pod-template-hash: "7469466749"
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: TZ
            value: UTC
          image: linuxserver/radarr:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /usr/bin/env
              - bash
              - -c
              - curl --fail localhost:7878/api/v3/system/status?apiKey=`IFS=\> &&
                while read -d \< E C; do if [[ $E = "ApiKey" ]]; then echo $C; fi;
                done < /config/config.xml`
            failureThreshold: 5
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          name: radarr
          ports:
          - containerPort: 7878
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 7878
            timeoutSeconds: 1
          resources: {}
          startupProbe:
            failureThreshold: 30
            periodSeconds: 5
            successThreshold: 1
            tcpSocket:
              port: 7878
            timeoutSeconds: 1
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /config
            name: config
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        volumes:
        - name: config
          persistentVolumeClaim:
            claimName: radarr-config
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: sonarr
      meta.helm.sh/release-namespace: media
    creationTimestamp: "2025-03-22T14:42:50Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: sonarr
      app.kubernetes.io/name: sonarr
      pod-template-hash: 6bc9d4f68f
    name: sonarr-6bc9d4f68f
    namespace: media
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: sonarr
      uid: 63ab095e-0d8a-4633-b823-c73d0188d5c6
    resourceVersion: "26349"
    uid: 349bb852-0ff8-4965-9132-dc8379a54f09
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: sonarr
        app.kubernetes.io/name: sonarr
        pod-template-hash: 6bc9d4f68f
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: sonarr
          app.kubernetes.io/name: sonarr
          pod-template-hash: 6bc9d4f68f
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: TZ
            value: UTC
          image: linuxserver/sonarr:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /usr/bin/env
              - bash
              - -c
              - curl --fail localhost:8989/api/v3/system/status?apiKey=`IFS=\> &&
                while read -d \< E C; do if [[ $E = "ApiKey" ]]; then echo $C; fi;
                done < /config/config.xml`
            failureThreshold: 5
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          name: sonarr
          ports:
          - containerPort: 8989
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 8989
            timeoutSeconds: 1
          resources: {}
          startupProbe:
            failureThreshold: 30
            periodSeconds: 5
            successThreshold: 1
            tcpSocket:
              port: 8989
            timeoutSeconds: 1
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /config
            name: config
          - mountPath: /data
            name: data
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        volumes:
        - name: config
          persistentVolumeClaim:
            claimName: sonarr-config
        - name: data
          persistentVolumeClaim:
            claimName: nfs-pvc
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: sonarr
      meta.helm.sh/release-namespace: media
    creationTimestamp: "2025-03-22T14:28:11Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: sonarr
      app.kubernetes.io/name: sonarr
      pod-template-hash: 7dc77cf8b8
    name: sonarr-7dc77cf8b8
    namespace: media
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: sonarr
      uid: 63ab095e-0d8a-4633-b823-c73d0188d5c6
    resourceVersion: "26276"
    uid: 6c229a00-cdfc-4c11-8ed7-6fa6cb669218
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: sonarr
        app.kubernetes.io/name: sonarr
        pod-template-hash: 7dc77cf8b8
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: sonarr
          app.kubernetes.io/name: sonarr
          pod-template-hash: 7dc77cf8b8
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: TZ
            value: UTC
          image: linuxserver/sonarr:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /usr/bin/env
              - bash
              - -c
              - curl --fail localhost:8989/api/v3/system/status?apiKey=`IFS=\> &&
                while read -d \< E C; do if [[ $E = "ApiKey" ]]; then echo $C; fi;
                done < /config/config.xml`
            failureThreshold: 5
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          name: sonarr
          ports:
          - containerPort: 8989
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 8989
            timeoutSeconds: 1
          resources: {}
          startupProbe:
            failureThreshold: 30
            periodSeconds: 5
            successThreshold: 1
            tcpSocket:
              port: 8989
            timeoutSeconds: 1
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /config
            name: config
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        volumes:
        - name: config
          persistentVolumeClaim:
            claimName: sonarr-config
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xWYY/aOBP+K68s9dObhCTAApH6IQfh4MpCFGh1p9MKGTMBH44d2Q4tWvHfT3bSbrZst1vpvtlm5snMPM/M8IhwST+BVFRwFKEd1uTYOQfIQSfK9yhCf4gdclABGu+xxih6RJhzobGmgitzFbt/gGgF2pNUeARrzcCjokON9xFY4RLBtRSMgXTJEUvtSjhQpaXFQM4PEcRnDtI9nE8NUOunc+D87wPl+/czYMXYgP4Uh+MCUIS0xJDT05vMVYmJ8TlVO3DVRWko0NVBRIINfUMLUBoXJYp4xZiDGN4Bs0Ux4dpclfdd5Pb1TXEcsTqiCBE/Hw0AD3DYh+4eyCAY9QdkgLthL8DdYd6Ffo6hn5vImhxt1SlXGjPmPn3oRyk5yKacQQ4SOAGFor+/08VN+ZGDdkyQ08p4ToCB5TLKMVPgoCfGvz01cmrzdcNIZTUDu67fDfbYzUM8dHt3MHRH0LtzA38Y4qAX7Pp5F10frg5SJRBT7R0mJ5HnC1pQjaLA930HaShKhjWY319R7ytECZ7Tw6wmYT2Lw/7d+2mYDIJBMBwNw7t+mEwnQS/w/aTfjcPhNJn4/VE47Y6CaRIPw2k4HfXupt1hrz/q/eeyubbSN9XGlINsiJMHc0CNAJCDXFeBdpWWlB+Qgw5M7DDzavYnkOOK6axuyct79OAg4GeL1BC0jO8T5KAzZlWbr6vzzeJTkq3nq2X7KUvSVfs+Sxb320k2/5RkLSwFRIJu241ncbbZmk+u03jc/u7zLnzu0DI7al2qqNN59/jh429Jtkw2yXobp/Pru44yxJO6lqrT5OGGA8/3Qj/4f1Wa403QLyS3ibPfk1+KMv64mW3TeL3ejrNkkiw383ixbrnZLmk7zJfrZPwxS7brD/N0u1msTRzz6V+v+aSLeL7czjab9DWr5WqbZqs/20ieOhPHI6xSGqTHBMHMCXyvF3q+53eCO3vpNpc21jSeL0yQ6WoxH7cRJXyV3/XBQbTAB/uKOTmC7JwYLUuQrhF5dPa9kddzdxVl+9AP+34QdFHjk1aMpYJRcjElyZdCpxIU8Nb4MBjIQRKUqKQdXo+mN4BUkurLWHANX7TtfMbE51TSM2VwgEQRzPDzqYVLvKOMampR0F6K0jRSvFggM24k4P2Ks0smhJ5SBg3LkZYVXB10Fqwq4F5UXNeNWJhjirUZIJ2jKOBZ3h2vibzJo/2bLfHP3AkmR7j1r5/fBGAn3AsI9fsNhC7K1tQuyluL7xGtGtQLds83gBlghtPrgxELF3tYAwOihTQ0mI6SHDQou50VihCjvPqCLCVKY6m/SWTFp5iySpq6vCABWfFYLQU3DNa8WTMiijKVIqfMrgt9Ke0Eq7imBTQDsp64IM+UQEyIyWbZ2rZPS6yWQa0AKEp9mVBZL6E9rQoUoXsohLy01vUN87/m9kT4L/o90fw2x6+UPyfX+TrEo8fm1NSl/q9XW7mtvWUXVk4P97g0Prxt3SjhyfwljVgmNNaV7fXrvwEAAP//At6ewcMKAAA
      objectset.rio.cattle.io/id: helm-controller-chart-registration
      objectset.rio.cattle.io/owner-gvk: helm.cattle.io/v1, Kind=HelmChart
      objectset.rio.cattle.io/owner-name: traefik
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-03-21T16:46:26Z"
    generation: 1
    labels:
      helmcharts.helm.cattle.io/chart: traefik
      objectset.rio.cattle.io/hash: c0f97ea7a25e3dec71957c7a3241a38f3e5fae5f
    name: helm-install-traefik
    namespace: kube-system
    ownerReferences:
    - apiVersion: helm.cattle.io/v1
      blockOwnerDeletion: false
      controller: false
      kind: HelmChart
      name: traefik
      uid: eb3031da-f2a8-46e8-9e46-1082a141b5f3
    resourceVersion: "665"
    uid: 1e5bfd3d-8df4-459b-8f92-2747ec21c2b6
  spec:
    backoffLimit: 1000
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 1e5bfd3d-8df4-459b-8f92-2747ec21c2b6
    suspend: false
    template:
      metadata:
        annotations:
          helmcharts.helm.cattle.io/configHash: SHA256=F2E71718982652EFD14100E53A28FED0592F391FEA82F2F946F3845947A2CD68
        creationTimestamp: null
        labels:
          batch.kubernetes.io/controller-uid: 1e5bfd3d-8df4-459b-8f92-2747ec21c2b6
          batch.kubernetes.io/job-name: helm-install-traefik
          controller-uid: 1e5bfd3d-8df4-459b-8f92-2747ec21c2b6
          helmcharts.helm.cattle.io/chart: traefik
          job-name: helm-install-traefik
      spec:
        containers:
        - args:
          - install
          - --set-string
          - global.systemDefaultRegistry=
          env:
          - name: NAME
            value: traefik
          - name: VERSION
          - name: REPO
          - name: HELM_DRIVER
            value: secret
          - name: CHART_NAMESPACE
            value: kube-system
          - name: CHART
            value: https://%{KUBERNETES_API}%/static/charts/traefik-27.0.201+up27.0.2.tgz
          - name: HELM_VERSION
          - name: TARGET_NAMESPACE
            value: kube-system
          - name: AUTH_PASS_CREDENTIALS
            value: "false"
          - name: INSECURE_SKIP_TLS_VERIFY
            value: "false"
          - name: PLAIN_HTTP
            value: "false"
          - name: NO_PROXY
            value: .svc,.cluster.local,10.42.0.0/16,10.43.0.0/16
          - name: FAILURE_POLICY
            value: reinstall
          image: rancher/klipper-helm:v0.9.4-build20250113
          imagePullPolicy: IfNotPresent
          name: helm
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /home/klipper-helm/.helm
            name: klipper-helm
          - mountPath: /home/klipper-helm/.cache
            name: klipper-cache
          - mountPath: /home/klipper-helm/.config
            name: klipper-config
          - mountPath: /tmp
            name: tmp
          - mountPath: /config
            name: values
          - mountPath: /chart
            name: content
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: helm-traefik
        serviceAccountName: helm-traefik
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
          name: klipper-helm
        - emptyDir:
            medium: Memory
          name: klipper-cache
        - emptyDir:
            medium: Memory
          name: klipper-config
        - emptyDir:
            medium: Memory
          name: tmp
        - name: values
          secret:
            defaultMode: 420
            secretName: chart-values-traefik
        - configMap:
            defaultMode: 420
            name: chart-content-traefik
          name: content
  status:
    completionTime: "2025-03-21T16:47:18Z"
    conditions:
    - lastProbeTime: "2025-03-21T16:47:18Z"
      lastTransitionTime: "2025-03-21T16:47:18Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: SuccessCriteriaMet
    - lastProbeTime: "2025-03-21T16:47:18Z"
      lastTransitionTime: "2025-03-21T16:47:18Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: Complete
    ready: 0
    startTime: "2025-03-21T16:46:28Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/7RWUY/aOBD+KydLfbokJCEsm0h9yLLh4MpCBLS602mFjJmAD8eObIcWrfjvJ5u0Dbt0u324N+LM92Vmvs8zPCFc0U8gFRUcJWiNNdl1DgFy0J7yDUrQn2KNHFSCxhusMUqeEOZcaKyp4Mo8ivW/QLQC7UkqPIK1ZuBR0aEGvQNWukRwLQVjIF2yw1K7ErZUaWk5kPNDBvGZg3S3h31D1Hp1CJzfPlC+eT8CVg4M6U95OC4BJUhLDAXdu0Ru3gRRFSYGt6/X4Kqj0lCik4OIBJv+kpagNC4rlPCaMQcxvAZmG2NStvUq71n29vTNueyw2qEERbdF0d30SDcI+utuPyzIhuCeH/fjXhEXYd8PceH34tBk19Rqu0+50pgx9/JjPyrNQbb0ORQggRNQKPnnmUdeSIEctGaC7GcGeQ8MrK5JgZkCB31X/9tRY622dlfVqa2HgrDYhH6/5xI/LNzoxu+56+5t7N52437fj9fQvemi0+PJQaoCYjq/xmQvimJCS6pREvi+7yANZcWwBvP+FTe/IprgBd2OzmIsRmnYu3mfde/8QRSF8e1wEAyCKE6Hd8NocBvHN8O7OIzCfppFQRbdRPFd3I0GaRT34ji46/8vFjq1WmC6jikH2Qgot+YHasyAHh0E/GBfNZ2fpg8ZctABs/q5ECfnW9SnbL4Yz6bto3mWz9rPo2zysLqfjz9l8xafAiJBt+MGo3S+XJnPLvJ00P725VW7BLTCdlpXKul03j19+HiXzafZMlus0nx8etdRRlFybpLqtGpxw77ne6Ef/F5XL5K+Utwynf+R/VKW6cflaJWni8VqMM/us+lynE4WLZi9Am3AeLrIBh/n2WrxYZyvlpOFyWM8/Ps1TD5Jx9PVaLnMX4uazlb5fPZXm8lTB+J4hNVKg/SYIJg5ge9Foed7fie4sQ/d5qHNNUzHE5NkPpuMB21GCV89dXp0EC3x1p5iTnYgO3tGqwqka9ybHHwv9iJ3XVO2Cf2w5wdBFzWYvGYsF4ySo2lJMRU6l6CAt2aD4UAOkqBELe1kejKGB1JLqo8DwTV80fZKMyY+55IeKIMtZIpghi9HEq7wmjKqqWVBGykqczvSyQSZOSIBb2acHedC6CFl0KicaFnDyUEHweoSHkTN9fl2leZnjrWZDJ2dKOGi7o7XZN7U0X5nW/wzOMFkBy/x5+M3EdjRdYXhfP6CQpdVaySX1cuI54zWDepK3OV4N1PJaHp6NGbhYgMLYEC0kEYGc6MkBw3KrmCFEsQor78gK4nSWOpvFpnxIaaslqYvVywga56qqeBGwbNuNoyIssqlKCize0AfKzvBaq5pCfdQ4Jrp8xgFeaAEUkJMNdPWOr3cUGcrnF0AZaWP91SeN8yG1iVK0AOUQh5bO/mF+r8G+y76L+K+S/024FfZLwV2vg7y5Kn51fTm/MfuHOU+W0p2GxV0+4Arg+NtROOIS8g1v1hVNNa1vfen/wIAAP//rlHlCbgKAAA
      objectset.rio.cattle.io/id: helm-controller-chart-registration
      objectset.rio.cattle.io/owner-gvk: helm.cattle.io/v1, Kind=HelmChart
      objectset.rio.cattle.io/owner-name: traefik-crd
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-03-21T16:46:26Z"
    generation: 1
    labels:
      helmcharts.helm.cattle.io/chart: traefik-crd
      objectset.rio.cattle.io/hash: 48ff3d5c3117b372fcdca509795f9f2702af0592
    name: helm-install-traefik-crd
    namespace: kube-system
    ownerReferences:
    - apiVersion: helm.cattle.io/v1
      blockOwnerDeletion: false
      controller: false
      kind: HelmChart
      name: traefik-crd
      uid: 12fd2075-c02f-4605-b389-8397709be363
    resourceVersion: "648"
    uid: d5a720d7-9a0f-45e6-a759-75c8bb879646
  spec:
    backoffLimit: 1000
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: d5a720d7-9a0f-45e6-a759-75c8bb879646
    suspend: false
    template:
      metadata:
        annotations:
          helmcharts.helm.cattle.io/configHash: SHA256=E3B0C44298FC1C149AFBF4C8996FB92427AE41E4649B934CA495991B7852B855
        creationTimestamp: null
        labels:
          batch.kubernetes.io/controller-uid: d5a720d7-9a0f-45e6-a759-75c8bb879646
          batch.kubernetes.io/job-name: helm-install-traefik-crd
          controller-uid: d5a720d7-9a0f-45e6-a759-75c8bb879646
          helmcharts.helm.cattle.io/chart: traefik-crd
          job-name: helm-install-traefik-crd
      spec:
        containers:
        - args:
          - install
          env:
          - name: NAME
            value: traefik-crd
          - name: VERSION
          - name: REPO
          - name: HELM_DRIVER
            value: secret
          - name: CHART_NAMESPACE
            value: kube-system
          - name: CHART
            value: https://%{KUBERNETES_API}%/static/charts/traefik-crd-27.0.201+up27.0.2.tgz
          - name: HELM_VERSION
          - name: TARGET_NAMESPACE
            value: kube-system
          - name: AUTH_PASS_CREDENTIALS
            value: "false"
          - name: INSECURE_SKIP_TLS_VERIFY
            value: "false"
          - name: PLAIN_HTTP
            value: "false"
          - name: NO_PROXY
            value: .svc,.cluster.local,10.42.0.0/16,10.43.0.0/16
          - name: FAILURE_POLICY
            value: reinstall
          image: rancher/klipper-helm:v0.9.4-build20250113
          imagePullPolicy: IfNotPresent
          name: helm
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /home/klipper-helm/.helm
            name: klipper-helm
          - mountPath: /home/klipper-helm/.cache
            name: klipper-cache
          - mountPath: /home/klipper-helm/.config
            name: klipper-config
          - mountPath: /tmp
            name: tmp
          - mountPath: /config
            name: values
          - mountPath: /chart
            name: content
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: helm-traefik-crd
        serviceAccountName: helm-traefik-crd
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
          name: klipper-helm
        - emptyDir:
            medium: Memory
          name: klipper-cache
        - emptyDir:
            medium: Memory
          name: klipper-config
        - emptyDir:
            medium: Memory
          name: tmp
        - name: values
          secret:
            defaultMode: 420
            secretName: chart-values-traefik-crd
        - configMap:
            defaultMode: 420
            name: chart-content-traefik-crd
          name: content
  status:
    completionTime: "2025-03-21T16:47:16Z"
    conditions:
    - lastProbeTime: "2025-03-21T16:47:16Z"
      lastTransitionTime: "2025-03-21T16:47:16Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: SuccessCriteriaMet
    - lastProbeTime: "2025-03-21T16:47:16Z"
      lastTransitionTime: "2025-03-21T16:47:16Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: Complete
    ready: 0
    startTime: "2025-03-21T16:46:28Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
kind: List
metadata:
  resourceVersion: ""
